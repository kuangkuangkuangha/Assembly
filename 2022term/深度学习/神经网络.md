### PyTorch中的自动微分

在torch中的torch.autograd模块，提供了实现任意标量值函数自动求导的类和函数。针对一个张量只需要设置参数requires_grad = True，通过相关计算即可输出其在传播过程中的梯度（导数）信息。

如在PyTorch中生成一个矩阵张量x xx，并且y = s u m ( x 2 + 4 x + 2 ) y=sum(x^2+4x+2)y=sum(x的2 +4x+2)，计算出y在x上的导数，实现过程如下：

```python
import torch

x = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
# 默认requires_grad=False
y = torch.sum(x**2+4*x+1)
print("x是否可导:", x.requires_grad)
print("y是否可导:", y.requires_grad)
print("x:", x)
print("y:", y)

>>>x是否可导: True
>>>y是否可导: True
>>>x: tensor([[1., 2.],
        [3., 4.]], requires_grad=True)
>>>y: tensor(74., grad_fn=<SumBackward0>)
```

上面程序中首先使用torch.tensor()函数生成一个矩阵x，并使用参数requires_grad=True来指定矩阵可以求导，然后根据公式y = sum( x的2 + 4 x + 2 ) 出标量y。

从输出的x.requires_grad和y.requires_grad的结果中可以看出，这两个变量都可以求导的（因为x xx可以求导，所以计算得到的y yy也可以求导）。

接下来利用y.backward()来计算y在x的每个元素上的导数

```python
y.backward()
print(x.grad)

>>>tensor([[ 6.,  8.],
        [10., 12.]])
```

通过y.backward()即可自动计算出y在x的每个元素上的导数，然后通过x的grad属性即可获取此时x的梯度，计算得到梯度值等于2x+4.




### 线性回归算法

比如要训练一个线性模型

h(x) = w0 + w1*x

如何得到最优的w0和w1？可以将它转化为体现预测值和真实值之间误差的一个误差函数

这里误差函数是一个均方误差函数。。。

![download](/Users/zhangkuang/Documents/College/2022term/深度学习/pic/download.png)



通过对该误差函数进行求梯度（就是求导）

可以计算得到w0和w1的极值，此时即为最优的参数w的解



### 梯度下降算法

梯度下降算法就是在前面的基础上进行优化，目标都是一样的，求解的思想有些不一样

：为了找到一个损失函数的局部最小值，必须向函数前点对应梯度（或者近视梯度）的反方向移动适当距离，从而实现迭代搜索。



最小化预测值与真实值之间的误差

->最小化误差函数

->损失函数：均方差函数，交叉熵函数





### 一个典型的神经网络训练过程包括以下几点：

- 1.定义一个包含可训练参数的神经网络

- 2.迭代整个输入

- 3.通过神经网络处理输入

- 4.计算损失(loss)

- 5.反向传播梯度到神经网络的参数

- 6.更新网络的参数，典型的用一个简单的更新方法：weight = weight - learning_rate *gradient



#### 定义神经网络

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


// 定义一个Net类
class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()		// 继承父类nn.Module
        # 1 input image channel, 6 output channels, 5x5 square convolution
        # kernel 数据成员
        self.conv1 = nn.Conv2d(1, 6, 5)			// 定义自己的卷积层参数conv1，conv2
        self.conv2 = nn.Conv2d(6, 16, 5)
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 5 * 5, 120)  // 定义自己的全连接层参数fc1，fc2， fc3
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

        // 成员函数 
        // 定义前馈函数
    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))  // 将自身池化两次
        # If the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

      // 定义成员函数 展平	
    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


net = Net()
print(net)
```

##### 

#### 处理输入以及调用反向传播

```python
// 处理输入
input = torch.randn(1, 1, 32, 32)   // 随机生成一个32*32的输入
out = net(input)
print(out)

// 调用反向传播
net.zero_grad()   // 清空现存的梯度
out.backward(torch.randn(1, 10))
```



当我们提到让网络学习时，实际上就是让损失函数的值最小

通过调整w0, w1, w2, w3, ..., wn和bias，使得损失函数的值最小，也就是将这些参数与输入的图片一起计算得到结果时，和我们预期得到结果的差距最小。因为我们定义损失函数时就是用的去差的平方值。

通过训练得到这些参数，就可以用到测试数据上了













神经网络按信息输入是否反馈，可以分为前馈神经网络（无反馈）和反馈神经网络（反向传播算法）

![前馈神经网络](/Users/zhangkuang/Documents/College/2022term/深度学习/pic/前馈神经网络.jpg)

![反馈神经网络](/Users/zhangkuang/Documents/College/2022term/深度学习/pic/反馈神经网络.jpg)